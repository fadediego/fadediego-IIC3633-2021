# Combining Predictions for Accurate Recommender Systems
La lectura escogida para esta semana fue el *paper* titulado *Combining Predictions for Accurate Recommender Systems*, escrito por Michael Jahrer, Andreas Toscher y Robert Legenstein. A modo de resumen, el texto habla sobre la ventaja del ensamble de modelos de *Collaborative Filtering* utilizando la técnica de *blending* para mezclar los distintos algoritmos. La idea es utilizar modelos de aprendizaje para aprender a mezclar correctamente los modelos de predicción. Los mejores resultados obtenidos sobre el conjunto de prueba de *Netflix Prize* se obtuvieron mediante el ensamble de Redes Neuronales, Regresiones Polinomiales y *Gradient Boosting Decision Trees*. Para aplicaciones prácticas, se propone el uso de Redes Neuronales entrenadas con *bagging* (una técnica de entrenamiento que consiste en entrenar N copias del modelo en conjuntos de entrenamiento levemente distintos), dado que su tiempo de entrenamiento es menor al ensamble de mejores resultados.

Hay al menos tres puntos interesantes del *paper* que vale la pena desarrollar y buscar posibles extensiones: (1) La elección de los modelos combinados, (2) el uso de *Gradient Boosting* para el *blending* más preciso, y (3) la utilización de *bagging* para entrenar los modelos de *blending*.

Sobre el primer punto, se menciona que la elección de los modelos utilizados para predicción de *ratings* está basada en aquellos modelos que obtuvieron los mejores resultados en el *dataset* de *Netflix Prize*. Lo que no se menciona es por qué se eligieron los distintos parámetros para cada modelo, y la razón con la cual se escoge el número de copias de cada modelo. Por ejemplo, el número de modelos de tipo SVD (factorización matricial) con distintos parámetros que se mezclan es 4, mientras que el número de RBM (Restricted Boltzmann Machines) utilizadas es 2. No queda claro si los 18 modelos que son mezclados son los 18 mejores modelos (en algún cierto orden) de la competencia *Netflix Prize*, o existe otro criterio. Dejando de lado aquella observación, sería interesante rediseñar el experimento anterior utilizando modelos más actuales como, por ejemplo, algún modelo de *Deep Learning* (dejando de lado las RBM, que tienen tan solo una capa oculta). Una opción sería incluir *AutoEncoders*, los cuales son modelos de gran profundidad, y que han sido utilizados sobre el *dataset* de *Netflix Prize* (véase, por ejemplo, [1]).

Sobre el segundo punto, se utiliza *Gradient Boosting* (GBoost) como uno de los modelos fundamentales para hacer *blending*. Una posible extensión a lo planteado por los autores en el *paper* leído, corresponde a utilizar XGBoost, una variante de GBoost, que se ha popularizado desde mediados de la decada de 2010 (posterior a la fecha de publicación del texto leído). Una de las ventajas de utilizar XGBoost en lugar de GBoost sería la rapidez con la que es optimizado, la cual es superior a la de GBoost. Dicha rapidez sería de gran ayuda para *datasets* de grandes, como el de *Netflix Prize* (véase [2] para mayor información de XGBoost y su optimización).

Sobre el último punto mencionado, en el *paper* se menciona la utilización de una técnica llamada *bagging*, la cual es aplicada para aumentar la precisión de los modelos basados en árboles de decisión, y consiste en entrenar múltiples copias de capa modelo sobre *datasets* levemente distintos, y promediar sus resultados (similar a la validación cruzada). Una extensión que sería interesante probar sería utilizar otras técnicas de entrenamiento para combinar los modelos, tales como *boosting* o *stacking* (más información de *bagging*, *boosting* y *stacking* en [3]). De todas formas, al utilizar *Gradient Boosting*, se realiza implícitamente la segunda técnica sobre los árboles de decisión.

Como conclusión, el *paper* leído entregó una interesante propuesta sobre el proceso de ensamble de algoritmos de recomendación, alcanzando niveles de precisión más altos que aquellos obtenidos en la competencia *Netflix Prize*. En el presente *blog* se discutieron tres puntos interesantes, junto a propuestas de extensión. En primer lugar, se planteó la inquietud sobre los modelos de recomendación escogidos, y se propuso una extensión del experimento que considera modelos más actuales, como AutoEncoders. Luego, se mencionó la idea de XGBoost como modelo de *blending*, el cual puede reemplazar al modelo GBoost presentado. Finalmente, se propuso la idea de utilizar otras técnicas similares a *bagging*, tales como *boosting* o *stacking*. Cualquiera sea el caso, los autores presentaron resultados sólidos con lo disponible en su época, y cualquier extensión requería de personas dispuestas a realizar una gran cantidad de esfuerzo en la investigación.

Fuentes:

[1] *Training Deep AutoEncoders for Collaborative Filtering*, Oleksii Kuchaiev, Boris Ginsburg (2017). Recuperado de: https://arxiv.org/abs/1708.01715

[2] *XGBoost: Extreme Gradient Boosting — How to Improve on Regular Gradient Boosting?*, Saul Dobilas (2021). Recuperado de: https://towardsdatascience.com/xgboost-extreme-gradient-boosting-how-to-improve-on-regular-gradient-boosting-5c6acf66c70a

[3] *Ensemble methods: bagging, boosting and stacking*, Joseph Rocca, 2019. Recuperado de: https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205