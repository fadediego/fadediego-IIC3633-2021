# Matrix Factorization Techniques for Recommender Systems + Netflix Update: Try This at Home

La lectura obligatoria de la semana consistía en el *paper* de Koren, Y., Bell, R., & Volinsky, C. llamado *Matrix Factorization Techniques for Recommender Systems*, el cual fue publicado el año 2009. Dicho *paper* explica brevemente la técnica de factorización matricial, bajo el contexto de la competencia *Netflix Prize*, de la cual estaban participando los autores. Además, por razones que se explicarán a continuación, lei el *blog* de Simon Funk, llamado *Netflix Update: Try This at Home*.

En términos generales, el *paper* me pareció conciso y fácil de entender, siendo de gran aporte los gráficos entregados (que de hecho presentaban la información de manera interesante). El hecho de que los autores hayan formado parte de la competencia de *Netflix* aporta un *insight* más directo y novedoso de cómo fue participar en una competencia así cuando el área de Sistemas Recomendadores estaba menos avanzado.

Si bien una buena parte de lo mencionado en el *paper* ya lo habíamos visto en CF, me pareció interesante la forma en que se calculan los vectores de factores (p y q), utilizando optimización de una función objetivo. Más aún, el hecho de que se puedan incluir en la función objetivo el sesgo, la dinámica temporal, y niveles de confianza de manera tan simple, me sorprendió positivamente.

Otro aspecto positivo que rescato del *paper* consiste en que, en la sección de *Learning Algorithms*, se explica de forma simple y precisa los algoritmos principales para optimizar la función objetivo. No conocía el método *Alternating Least Squares* y, si bien no parece tener importancia en el desarrollo del texto, me hubiera gustado que explicaran más detalladamente por qué dicho algoritmo puede ser paralelizable.

En la misma línea, un aspecto del *paper* que no me gustó es el hecho de que no hayan recordado (ni siquiera brevemente) qué es la descomposición de valores singulares original, y cuál es su manera de obtenerla (por ejemplo, utilizando una fórmula). Por dicha razón, me animé a leer el *blog* de Simon Funk, en el cual se introduce el FunkSVD, esperando que explicara dicha descomposición. Lamentablemente, en dicho *blog* tampoco se muestra un repaso de SVD original. De todas formas, si se entregan explicaciones más detalladas de dónde viene la matemática detrás del método FunkSVD, mencionando aspectos como distribuciones *a priori* (de inferencia bayesiana) y la regularización de Tikhonov (de álgebra lineal).

En conclusión, el *paper* *Matrix Factorization Techniques for Recommender Systems* me pareció entretenido y conciso. Valoré aspectos tales como la narración del problema de *Netflix Prize* desde primera fuente. En algunos pasajes del texto me hubiera gustado tener una explicación matemática más detallada, sin embargo, imagino que ese no era el objetivo de los autores. El *blog* de Simon Funk me pareció interesante, sobre todo por las implicancias que causó (sin siquiera ser una publicación en algún medio importante). Lamentablemente, no pude encontrar lo que buscaba en dicho *blog*, pero de todas formas, aprendí parte de la teoría detrás del modelo presentado en el *paper*.